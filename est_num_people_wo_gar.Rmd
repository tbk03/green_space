---
title: "Estimating the number of people without a garden"
output: html_notebook
---

```{r}
library(tidyverse)

library(janitor)    # for data cleaning
library(readxl)     # for readig excel files
library(visdat)     # for a quick look at data quality
```

```{r}
theme_set(theme_light())
```

## Processing the data

### Gardens data

Dataset downloaded from [ONS](https://www.ons.gov.uk/economy/environmentalaccounts/datasets/accesstogardensandpublicgreenspaceingreatbritain) on 5th June 2021. Both gardens space and public parks datasets are the April 2020 versions.

```{r}
# a convenience function for renaming columns
# I am sure there is a better way to do this!
replace_in_column_name <- function(df, pattern, replacement){
  df %>% 
    rename_with(.fn = ~ str_replace(.x, pattern, replacement))
}

# read in and clean data
gardens <- read_xlsx(
  "data/osprivateoutdoorspacereferencetables_edited_for_import.xlsx",
  sheet = "MSOA gardens", 
  skip = 1) %>% 
  
  # make variables names more conistent
  janitor::clean_names() %>% 
  
  # focus on variables of interest 
  select(country_code:msoa_name,
         ends_with("count") |
         starts_with("total")) %>% 
  
  # simplify variables names (now data by housing type has been removed)
  replace_in_column_name("total_", "") %>% 
  replace_in_column_name("private_outdoor_space", "gar") %>% 
  replace_in_column_name("address", "ad") %>% 
  replace_in_column_name("adress", "ad") %>%
  replace_in_column_name("adresses", "ads") %>% 
  replace_in_column_name("percentage", "perc") %>% 
  replace_in_column_name("_m2", "") %>% 
  replace_in_column_name("average", "ave")

# display data for quick visual checks
gardens
  
  
```

```{r}
# get a quick visual summary of data types and the amount of missing data
gardens %>% 
  visdat::vis_dat()

gardens %>% 
  visdat::vis_miss()
```

### Indicies of multiple deprivation

ONS released the 2019 IMD data at LSOA scale. MySociety have produced [IMD at various other scales](https://research.mysociety.org/sites/imd2019/about/) including the MSOA scale.

```{r}
imd_2019 <- read_csv("data/imd2019_msoa_level_data.csv")
imd_2019
```

### Population data

ONS provide [MSOA level population data](https://www.ons.gov.uk/peoplepopulationandcommunity/populationandmigration/populationestimates/datasets/middlesuperoutputareamidyearpopulationestimates). Here I use the most recent release (mid 2019).

```{r}
msoa_pops <- read_xlsx(
  
  "data/SAPE22DT4-mid-2019-msoa-syoa-estimates-unformatted.xlsx",
  sheet = "Mid-2019 Persons",
  skip = 4) %>% 
  
  # process variable names for conistency
  janitor::clean_names() %>% 
  
  # select minimal number of columns
  select(msoa_code, population = all_ages)

msoa_pops %>% 
  visdat::vis_miss()
```

### Transforming the data for plotting

Overall average = 2.4 (2020 see [here](https://www.ons.gov.uk/peoplepopulationandcommunity/birthsdeathsandmarriages/families/bulletins/familiesandhouseholds/2020))

2017-18 was the most year I could find on [average occupancy figures broken down by house/flat from MCHLG](https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/817286/EHS_2017-18_Households_Report.pdf):

-   House = 2.5 people

-   High-rise flat = 1.9

-   Mid-rise flat = 1.8

More details [here](https://www.gov.uk/government/statistics/english-housing-survey-2017-to-2018-households).

So for now, I'll make a conservative assumption that the average occupancy for a flat is 1.8 (i.e. the lower of the mid and high rise figures above).

**I looked at trying to estimate more accurately, but the figures for the number of dwellings per block are not available.**

2020 estimates of the numbers of high and mid rise flat can be found [here](https://www.gov.uk/government/publications/building-safety-programme-estimates-of-ews1-requirements-on-residential-buildings-in-england/building-safety-programme-estimates-of-ews1-requirements-on-residential-buildings-in-england). These can be used to calculate an average flat occupancy rate.

-   12,500 blocks of high rise flats

-   77,500 blocks of mid rise flats

```{r}
# calculate scaling factors for ave occupancy in an MSOA
# from national data
nat_ave_occ <- 2.4
nat_house_ave_occ <- 2.5
nat_flat_ave_occ <- 1.9

flat_scale <- nat_flat_ave_occ / nat_ave_occ
house_scale <- nat_house_ave_occ / nat_ave_occ


people_wo_gar <- gardens %>% 
  left_join(msoa_pops) %>% 
  mutate(house_ad_without_gar_count = houses_ad_count - houses_ad_with_gar_count,
         flats_ad_without_gar_count = flats_ad_count - flats_ad_with_gar_count,
         ave_occ = population / ad_count,
         ave_occ_flat = flat_scale * ave_occ,
         ave_occ_house = house_scale * ave_occ,
         pop_calc = (ave_occ_flat * flats_ad_count) + (ave_occ_house * houses_ad_count),
         pop_diff = population - pop_calc) 

ggplot(people_wo_gar, aes(pop_diff)) +
  geom_histogram()
  
```

May be more accurate to just use average occupancy rates for each MSOA??

```{r}
people_wo_gar <- gardens %>% 
  left_join(msoa_pops) %>% 
  mutate(ad_wo_gar = ad_count * (1 - perc_of_ades_with_gar),
         ave_occ = population / ad_count,
         people_wo_gar = round(ad_wo_gar * ave_occ)) %>% 
  select(country_code:msoa_name, people_wo_gar)

people_wo_gar

ggplot(people_wo_gar, aes(people_wo_gar)) +
  geom_histogram()

```

<https://www.robert-hickman.eu/post/getis-ord-heatmaps-tutorial/>

```{r}
library(sf)

# focus on Englnad and Wales
# (as shapefile for MSOAs only inlcudes England and Wales)
people_wo_gar_EW <- people_wo_gar %>% 
  filter(country_name == "England" |
         country_name == "Wales")

median_msoa_peop_wo_gar = median(people_wo_gar_EW$people_wo_gar, na.rm = TRUE)


# read in MSOA boundary shapefile
msoa_bound <- st_read("data/MSOA_bound/Middle_Layer_Super_Output_Areas_(December_2011)_Boundaries.shp") %>% 
  select(msoa_code = msoa11cd, 
         msoa_name = msoa11nm)

# read in countries boundary shapefile (for clipping polygon grid)
country_bound <- st_read("data/Countries_bound/Countries_(December_2017)_Boundaries.shp") %>% 
  filter(ctry17nm == "England" | ctry17nm == "Wales")

# join garden data with
people_wo_gar_spatial <- msoa_bound %>% 
  left_join(people_wo_gar_EW) %>% 
  
  # median imputation for one msoa with missing
  mutate(people_wo_gar = replace_na(people_wo_gar, median_msoa_peop_wo_gar))
  


# ###########################
# simulate the locations of people without a garden
# ###########################

# random locations withi an MSOA was too computationally intesnive
# so I went with placing the 'people' at MSOA centroids
people <- people_wo_gar_spatial %>% 
  mutate(people_wo_gar = round(people_wo_gar / 10),
         geometry = st_centroid(geometry)) %>% 
  uncount(people_wo_gar)


sum(people_wo_gar_spatial$people_wo_gar)
# create boundary file
map_bound <- people_wo_gar_spatial %>%
  summarise()# %>% 
  #st_transform(4326)

# people <- st_sample(
#   select(people_wo_gar_spatial, -people_wo_gar),
#   size = round(people_wo_gar_spatial$people_wo_gar / 100))

# ###########################
# create hexagonal grid
# ###########################

hex_polygons <- st_make_grid(map_bound, 7500,
                             crs = st_crs(people_wo_gar_spatial),
                             what = "polygons",
                             square = FALSE) %>% 
  st_sf()

# calculate number of people in each polygon
intersects <- st_intersects(hex_polygons, people)
hex_polygons$people_wo_gar <- lengths(intersects)

# ###########################
# create the plot
# ###########################
# ggplot(people_wo_gar_spatial) +
#   #geom_sf(data = map_bound, colour = "black", size = 1) +
#   geom_sf(data = hex_polygons, fill = "black") + # , aes(fill = people_wo_gar)
#   scale_fill_viridis_c(trans = "log", direction = -1) +
#   #scale_color_viridis_c(trans = "log") +
#   ggthemes::theme_map()

# crop hexogens to outline of England and Wales
hex_polygons_EW <- hex_polygons[country_bound, ]

# hex_polygon_EW %>% 
#   count(people_wo_gar)

ggplot() +
  #geom_sf(data = map_bound) +
  #geom_sf(data = st_centroid(msoa_bound), colour = "grey") +
  geom_sf(data = hex_polygons_EW, alpha = 0.5, mapping = aes(fill = people_wo_gar)) +
  scale_fill_viridis_c(trans = "log", direction = -1) +
  theme_void()
  
# hex_polygons %>% 
#   filter(people_wo_gar == 0)
# 
# people_wo_gar_spatial %>% 
#   filter(people_wo_gar == 0)

# ###########################
# Smooth
# ###########################



```

<https://pudding.cool/process/regional_smoothing/>

```{r}
library(spdep)

# convert geo-df to plain df
hex_polygons_EW_df <- hex_polygons_EW %>% 
  st_drop_geometry()

# get centroid of each hexagon
hex_centroids <- hex_polygons_EW %>% 
  mutate(geometry = st_centroid(geometry))

# find k nearest neighbors
knn5 <- knn2nb(knearneigh(hex_centroids, k = 10))
knn5 <- include.self(knn5)

# Creating the localG statistic for each of counties, with a k-nearest neighbor value of 5, and round this to 3 decimal places
localGvalues <- localG(x = as.numeric(hex_polygons_EW_df$people_wo_gar), listw = nb2listw(knn5, style = "B"), zero.policy = TRUE)
localGvalues <- round(localGvalues,3)

# Create a new data frame that only includes the hexogons and the G scores
new_df <- hex_polygons_EW 
new_df$values <- localGvalues

new_df %>% 
  ggplot(aes(values + 1.5)) +
  geom_histogram()

sum(new_df$values == 0)

ggplot() +
  #geom_sf(data = map_bound) +
  #geom_sf(data = st_centroid(msoa_bound), colour = "grey") +
  geom_sf(data = new_df, mapping = aes(fill = values + 1.5,
                                       colour = values + 1.5)) +
  scale_fill_viridis_c(trans = "log", option = "magma", begin = 0) +
  scale_colour_viridis_c(trans = "log", option = "magma", begin = 0) +
  theme_void() +
  theme(panel.background = element_rect(fill = "white"))
```

Looking at change in park usage (in two periods in 2020) vs garden access.

[Usage data from Google via ONS](https://www.ons.gov.uk/economy/environmentalaccounts/articles/howhaslockdownchangedourrelationshipwithnature/2021-04-26)

```{r}

# read in Google/ONS park usage data
park_useage <- read_xlsx("data/change_in_parks_visits_2020_LA.xlsx", skip = 6) %>%
  
  # apply consistent naming style
  janitor::clean_names() %>% 
  
  # shorten names for readiblity
  rename(perc_ch_parks_spring_lock = percent_change_in_visits_to_and_time_spent_in_parks_during_spring_2020_lockdown,
         perc_ch_park_summer = percent_change_in_visits_to_and_time_spent_in_parks_during_july_and_august_2020,
         lad_code = area_codes,
         lad_name = area_names)

park_useage

# read in urban / rural classification
urb_rur_class <- read_csv("data/Rural_Urban_Classification_(2001)_for_MSOAs_in_England_and_Wales/RUC_MSOA_2001_EW_LU.csv") %>% 
  clean_names() %>% 
  select(lad_code = lad01cd, lad_name = lad01nm, 
         class_name = morphology_name, 
         class_code = morphology_code)

# process urban / rural classification 
urb_rur_LAD <- urb_rur_class %>% 
  mutate(is_urban = class_code == 1) %>% 
  group_by(lad_name) %>% 
  summarise(n_tot = n(),
            n_urban = sum(is_urban),
            prop_urban = n_urban / n_tot) %>% 
  mutate(is_urban_LAD = prop_urban > 0.75)

# read in LA boundaries
LA_bounds <- st_read("data/LA_bound/Local_Authority_Districts_(December_2019)_Boundaries_UK_BFC.shp") %>% 
  rename(lad_code = lad19cd,
         lad_name = lad19nm )

LA_bounds

# aggregate gardens data to LAD scale
gardens_LAD <- gardens %>% 
  mutate(ad_wo_gar = round(ad_count * (1 - perc_of_ades_with_gar))) %>% 
  group_by(lad_code, lad_name) %>% 
  summarise(ad_count = sum(ad_count),
            ad_wo_gar = sum(ad_wo_gar),
            prop_ad_wo_gar = ad_wo_gar / ad_count) 

# join dat sets
gar_park_use <- park_useage %>% 
  left_join(gardens_LAD) %>% 
  left_join(urb_rur_LAD)

# check for NA created during joining
gar_park_use %>% 
  skimr::skim()

gar_park_use %>% 
  filter(is.na(is_urban_LAD))

# produce plot
ggplot(gar_park_use, aes(prop_ad_wo_gar, perc_ch_parks_spring_lock,
                         colour = is_urban_LAD)) +
  geom_point() +
  geom_smooth(method = "lm") +
  #scale_x_log10() +
  xlim(c(0,0.5)) +
  facet_wrap(~is_urban_LAD)

ggplot(gar_park_use, aes(prop_ad_wo_gar, perc_ch_park_summer)) +
  geom_point()

mod_1 <- rstanarm::stan_glm(perc_ch_parks_spring_lock ~ prop_ad_wo_gar, 
                            data = filter(gar_park_use, 
                                          prop_ad_wo_gar < 0.2 &
                                            is_urban_LAD == TRUE), 
                            refresh = 0)

median(rstanarm::bayes_R2(mod_1))
```

```{r}
foe_green_space <- read_xlsx("data/(FOE) Green Space Consolidated Data - England - Version 2.1.xlsx",
                             sheet = "Local Authorities V2.1") %>% 
  clean_names() %>% 
  select(lad_code = la_code, 
         lad_name = la_name,
         pop = total_pop_from_ethnicity_data,
         bame_pop,
         income_index,
         pcnt_pop_with_go_space_access,
         green_space_area_per_capita)

# join foe data to existing data
gar_park_use_foe <- gar_park_use %>% 
  left_join(foe_green_space)

# check NAs created by joining
skimr::skim(gar_park_use_foe)
gar_park_use_foe %>% 
  filter(is.na(pop))  # almost all Scotland and Wales 
                      # (as expected as FOE is England only)


gar_park_use_foe_eng <- gar_park_use_foe %>% 
  filter(!is.na(pop)) %>% 
  mutate(perc_ch_park_summer = na_if(perc_ch_park_summer, "No data"),
         perc_ch_park_summer = as.numeric(perc_ch_park_summer),
         prop_bame_pop = bame_pop / pop)

gar_park_use_foe_eng

gar_park_use_foe_eng_mod_in <- gar_park_use_foe_eng %>% 
  select(-lad_code, -lad_name, -pop, -bame_pop,
         -n_tot, -n_urban, -is_urban_LAD, -ad_count,-ad_wo_gar,
         -perc_ch_park_summer, -pcnt_pop_with_go_space_access, -green_space_area_per_capita)
  

mod_2 <- rstanarm::stan_glm(perc_ch_parks_spring_lock ~ ., 
                            data = gar_park_use_foe_eng_mod_in, 
                            refresh = 0)

median(rstanarm::bayes_R2(mod_2))

mod_3 <- rstanarm::stan_glm(perc_ch_parks_spring_lock ~ prop_ad_wo_gar + prop_urban + income_index + prop_bame_pop, 
                            data = gar_park_use_foe_eng_mod_in, 
                            refresh = 0)

median(rstanarm::bayes_R2(mod_3))

ggplot(gar_park_use_foe_eng_mod_in, aes(prop_bame_pop, perc_ch_parks_spring_lock)) +
  geom_point()

p <- ggplot(filter(gar_park_use_foe_eng, prop_urban > 0.8), 
       aes(prop_ad_wo_gar, perc_ch_parks_spring_lock, 
           colour = prop_bame_pop, 
           size = green_space_area_per_capita,
           label = lad_name)) +
  geom_point(alpha = 0.5) +
  scale_colour_viridis_c() +
  xlim(c(0,.5))
  #scale_x_log10()

plotly::ggplotly(p)

p2 <- ggplot(data = filter(gar_park_use_foe_eng, prop_urban > 0.8),
             aes(prop_bame_pop, prop_ad_wo_gar)) +
  geom_point() +
  ylim(c(0,0.5))

p2

p3 <- ggplot(filter(gar_park_use_foe_eng, prop_urban > 0.8), 
       aes(prop_bame_pop, perc_ch_parks_spring_lock, 
           colour = prop_ad_wo_gar, 
           size = green_space_area_per_capita,
           label = lad_name)) +
  geom_point(alpha = 0.5) +
  scale_colour_viridis_c(direction = -1, trans = "log", end = 0.9) +
  xlim(c(0,.5))

p3
plotly::ggplotly(p3)

p4 <- ggplot(filter(gar_park_use_foe_eng, prop_urban > 0.8), 
       aes(perc_ch_parks_spring_lock, perc_ch_park_summer,
           colour = prop_ad_wo_gar,
           size = green_space_area_per_capita,
           label = lad_name)) +
  geom_point(alpha = 0.5)+
  scale_colour_viridis_c(direction = -1, trans = "log", end = 0.9)

p4

plotly::ggplotly(p4)
```

Major cities

```{r}
la_region_lookup <- gardens %>%
  select(region_code:lad_name) %>% 
  distinct()

london <- gar_park_use_foe_eng %>% 
  left_join(la_region_lookup) %>% 
  filter(region_name == "London")

other_big_cities <- gar_park_use_foe_eng %>% 
  left_join(la_region_lookup) %>% 
  filter(region_name != "London") %>% 
  filter(prop_urban >= 0.8) %>% 
  slice_max(order_by = pop, n = 29)

thirty_biggest_cities <- london %>% 
  bind_rows(other_big_cities) %>% 
  mutate(is_london = region_name == "London",
         gs_pc_bin = cut(green_space_area_per_capita,
                         breaks = c(0, 10, 25, 50, Inf)),
         prop_ad_with_gar = 1 - prop_ad_wo_gar)

colours <- c("grey30", "grey70", "palegreen", "palegreen4")

p <- ggplot(thirty_biggest_cities, 
       aes(prop_ad_with_gar, perc_ch_parks_spring_lock, 
           colour = gs_pc_bin, 
           size = gs_pc_bin,
           label = lad_name)) +
  geom_point(alpha = 0.5) +
  scale_colour_manual(values = colours) +
  #scale_colour_viridis_c() +
  #scale_colour_gradient(low = "grey30", high = "green") +
  xlim(c(0.5,1)) +
  facet_wrap(~is_london)
  #scale_x_log10()

plotly::ggplotly(p)

ggplot(thirty_biggest_cities) +
  geom_histogram(mapping = aes(green_space_area_per_capita))

p4 <- ggplot(thirty_biggest_cities, 
       aes(perc_ch_parks_spring_lock, perc_ch_park_summer,
           colour = green_space_area_per_capita,
           size = green_space_area_per_capita,
           label = lad_name)) +
  geom_point(alpha = 0.5)+
  scale_colour_viridis_c(end = 0.9) +
  facet_wrap(~is_london)

p4

plotly::ggplotly(p4)
```

```{r}
library(nngeo)

# identify nearest neighbours and convert alogithmn output to a dataframe
neigh_hex <- st_nn(hex_polygons_EW, hex_polygons_EW, k = 6)
neigh_hex <- data.frame(matrix(unlist(neigh_hex), ncol = max(lengths(neigh_hex)), byrow = TRUE))

look_up_peop_wo_gar <- function(df, row_num){
  df[row_num, 'people_wo_gar'] %>% 
    st_drop_geometry() %>% 
    pluck('people_wo_gar')
}


# reset row numbers to avoid confusion when adding nearest neighbors
row.names(hex_polygon_EW) <- NULL

# add nearest neighbors to polygon
hex_polygon_EW_nn <- hex_polygon_EW %>% 
  bind_cols(neigh_hex)


knn_interpolation <- hex_polygon_EW_nn %>% 
  mutate(across(X1:X6, ~ look_up_peop_wo_gar(hex_polygon_EW_nn, .))) %>% 
  st_drop_geometry() %>% 
  mutate(knn_ave = round(rowMeans(select(., starts_with("X"))))) %>% 
  pull(knn_ave)
  
hex_polygon_EW_nn %>% 
  bind_cols(knn_interpolation)

hex_polygon_interpolation
```
